{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Definition\n",
    "\n",
    "Here I defined the TicTacToe class, explaining the methods:\n",
    "\n",
    "- `print_board_nice()` -> pretty self explanatory to be honest\n",
    "- `state()` -> returns the current state, described as the flattened board\n",
    "- `next_actions()` -> returns the next possible actions, described as row and column\n",
    "- `make_move()` -> assigns the player index to the chosen place of the board\n",
    "- `check_winner()` -> also pretty self explanatory\n",
    "- `reward()` -> rewards the player after a win and penalizes after a loss\n",
    "- `is_over()` -> checks if a game is over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "  def __init__(self):\n",
    "    self.board = np.zeros((3,3), dtype=np.int8)\n",
    "    self.player = 1\n",
    "    self.MAGIC = np.array([[1,6,5],[8,4,0],[3,2,7]])\n",
    "    self.winner = None\n",
    "\n",
    "  def print_board_nice(self):\n",
    "    for i in range(3):\n",
    "      for j in range(3):\n",
    "        if self.board[i,j] == 1:\n",
    "          print(\"X\", end=\"\")\n",
    "        elif self.board[i,j] == -1:\n",
    "          print(\"O\", end=\"\")\n",
    "        else:\n",
    "          print(\" \", end=\"\")\n",
    "        if j != 2:\n",
    "          print(\"|\", end=\"\")\n",
    "      print()\n",
    "      if i != 2:\n",
    "        print(\"-----\")\n",
    "    print()\n",
    "  \n",
    "  def state(self):\n",
    "    return self.board.flatten()\n",
    "  \n",
    "  def next_actions(self):\n",
    "    if self.winner is not None:\n",
    "      return list()\n",
    "    row, col = np.where(self.board == 0)\n",
    "    return list(zip(row, col))\n",
    "  \n",
    "  def make_move(self, move):\n",
    "    if self.board[move] != 0:\n",
    "      raise ValueError(\"Invalid move\")\n",
    "    self.board[move] = self.player\n",
    "    self.player *= -1\n",
    "\n",
    "  def check_winner(self, player):\n",
    "    cells = self.MAGIC[self.board == player]\n",
    "    if any(sum(cells) == 12 for cells in permutations(cells, 3)):\n",
    "      self.winner = player\n",
    "      return True\n",
    "    return False\n",
    "  \n",
    "  def reward(self, player):\n",
    "    if self.check_winner(player):\n",
    "      return 1\n",
    "    if self.check_winner(-player):\n",
    "      return -1\n",
    "    return 0\n",
    "\n",
    "  def is_over(self):\n",
    "    return len(self.next_actions()) == 0 or self.check_winner(1) or self.check_winner(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I defined the Q-Learning class which is pretty similar to the standard approach.\n",
    "\n",
    "To balance exploration and exploitation I used an epsilon variable that decreases linearly with the number of iterations, so exploration is favored in the beginning, while exploitation is favored in the end of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "  def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    self.alpha = alpha\n",
    "    self.gamma = gamma\n",
    "    self.epsilon = epsilon\n",
    "    self.Q = dict()\n",
    "\n",
    "  def set_epsilon(self, epsilon):\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def get_Q(self, state, action):\n",
    "    if (state, action) not in self.Q:\n",
    "      self.Q[(state, action)] = 0\n",
    "    return self.Q[(state, action)]\n",
    "\n",
    "  def get_action(self, state, actions):\n",
    "    if np.random.uniform() < self.epsilon:\n",
    "      return actions[np.random.choice(range(len(actions)))]\n",
    "    else:\n",
    "      Qs = np.array([self.get_Q(state, action) for action in actions])\n",
    "      max_Q = np.max(Qs)\n",
    "      return actions[np.random.choice(np.where(Qs == max_Q)[0])]\n",
    "\n",
    "  def update(self, state, action, reward, next_state, next_actions):\n",
    "    q_value = self.get_Q(state, action)\n",
    "    next_q_values = np.array([self.get_Q(next_state, next_action) for next_action in next_actions])\n",
    "    max_next_q_value = np.max(next_q_values) if len(next_q_values) > 0 else 0\n",
    "    self.Q[(state, action)] = q_value + self.alpha * (reward + self.gamma * max_next_q_value - q_value)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 76/50000 [00:00<02:12, 377.90game/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50000/50000 [01:32<00:00, 537.66game/s]\n"
     ]
    }
   ],
   "source": [
    "Q1 = QLearning()\n",
    "games = 50000\n",
    "epsilon = np.linspace(1, 0.1, num=games, endpoint=True)\n",
    "\n",
    "for i in tqdm(range(games), desc=\"Training\", unit=\"game\"):\n",
    "  game = TicTacToe()\n",
    "  Q1.set_epsilon(epsilon[i])\n",
    "  while not game.is_over():\n",
    "    state = game.state().copy()\n",
    "    actions = game.next_actions()\n",
    "    action = Q1.get_action(str(state), actions)\n",
    "    game.make_move(action)\n",
    "    \n",
    "    if game.is_over():\n",
    "      next_state = game.state().copy()\n",
    "      next_actions = game.next_actions()\n",
    "      reward = game.reward(1)\n",
    "      Q1.update(str(state), action, reward, str(next_state), next_actions)\n",
    "\n",
    "    else:\n",
    "      reward = game.reward(1)\n",
    "      actions_2 = game.next_actions()\n",
    "      action_2 = actions_2[np.random.choice(range(len(actions_2)))]\n",
    "      game.make_move(action_2)\n",
    "\n",
    "      if game.is_over():\n",
    "        reward = game.reward(1)\n",
    "        \n",
    "      next_state = game.state().copy()\n",
    "      next_actions = game.next_actions()\n",
    "      Q1.update(str(state), action, reward, str(next_state), next_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins:  9955\n",
      "Losses:  0\n",
      "Ties:  45\n",
      "Win rate:  0.9955\n",
      "Loss rate:  0.0\n",
      "Tie rate:  0.0045\n"
     ]
    }
   ],
   "source": [
    "Q1.set_epsilon(0)\n",
    "wins = 0\n",
    "losses = 0\n",
    "ties = 0 \n",
    "games = 10000\n",
    "\n",
    "for i in range (games):\n",
    "  game = TicTacToe()\n",
    "  while not game.is_over():\n",
    "    if game.player == 1:\n",
    "      state = game.state()\n",
    "      actions = game.next_actions()\n",
    "      action = Q1.get_action(str(state), actions)\n",
    "      game.make_move(action)\n",
    "    else:\n",
    "      state = game.state()\n",
    "      actions = game.next_actions()\n",
    "      action = actions[np.random.choice(range(len(actions)))]\n",
    "      game.make_move(action)\n",
    "    \n",
    "  if game.winner == 1:\n",
    "    wins += 1\n",
    "  elif game.winner == -1:\n",
    "    losses += 1\n",
    "  else:\n",
    "    ties += 1\n",
    "\n",
    "print(\"Wins: \", wins)\n",
    "print(\"Losses: \", losses)\n",
    "print(\"Ties: \", ties)\n",
    "print(\"Win rate: \", wins/games)\n",
    "print(\"Loss rate: \", losses/games)\n",
    "print(\"Tie rate: \", ties/games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach can be used to create an agent that plays as player 2, however the results may vary slightly because of the nature of the Tic Tac Toe game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
